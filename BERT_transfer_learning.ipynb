{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6cCZLYz8hyL3",
    "outputId": "cee4bf59-9701-4c29-adcf-df8cbfbf6438",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kashgari<2.0,>=1.1 in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
      "Requirement already satisfied: keras-bert>=0.50.0 in /usr/local/lib/python3.6/dist-packages (from kashgari<2.0,>=1.1) (0.86.0)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from kashgari<2.0,>=1.1) (1.1.5)\n",
      "Requirement already satisfied: gensim>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from kashgari<2.0,>=1.1) (3.8.3)\n",
      "Requirement already satisfied: bert4keras==0.6.5 in /usr/local/lib/python3.6/dist-packages (from kashgari<2.0,>=1.1) (0.6.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from kashgari<2.0,>=1.1) (0.24.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from kashgari<2.0,>=1.1) (2.10.0)\n",
      "Requirement already satisfied: numpy==1.16.4 in /usr/local/lib/python3.6/dist-packages (from kashgari<2.0,>=1.1) (1.16.4)\n",
      "Requirement already satisfied: seqeval==0.0.10 in /usr/local/lib/python3.6/dist-packages (from kashgari<2.0,>=1.1) (0.0.10)\n",
      "Requirement already satisfied: keras-gpt-2>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from kashgari<2.0,>=1.1) (0.15.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from bert4keras==0.6.5->kashgari<2.0,>=1.1) (2.4.3)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.5.0->kashgari<2.0,>=1.1) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.5.0->kashgari<2.0,>=1.1) (1.5.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.5.0->kashgari<2.0,>=1.1) (4.2.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->bert4keras==0.6.5->kashgari<2.0,>=1.1) (5.4.1)\n",
      "Requirement already satisfied: keras-transformer>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert>=0.50.0->kashgari<2.0,>=1.1) (0.38.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from keras-gpt-2>=0.8.0->kashgari<2.0,>=1.1) (2020.11.13)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras-bert>=0.50.0->kashgari<2.0,>=1.1) (0.6.0)\n",
      "Requirement already satisfied: keras-layer-normalization>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras-bert>=0.50.0->kashgari<2.0,>=1.1) (0.14.0)\n",
      "Requirement already satisfied: keras-multi-head>=0.27.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras-bert>=0.50.0->kashgari<2.0,>=1.1) (0.27.0)\n",
      "Requirement already satisfied: keras-embed-sim>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras-bert>=0.50.0->kashgari<2.0,>=1.1) (0.8.0)\n",
      "Requirement already satisfied: keras-pos-embd>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras-bert>=0.50.0->kashgari<2.0,>=1.1) (0.11.0)\n",
      "Requirement already satisfied: keras-self-attention==0.46.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.27.0->keras-transformer>=0.38.0->keras-bert>=0.50.0->kashgari<2.0,>=1.1) (0.46.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari<2.0,>=1.1) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->kashgari<2.0,>=1.1) (2.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari<2.0,>=1.1) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.1->kashgari<2.0,>=1.1) (1.0.1)\n",
      "Requirement already satisfied: tensorflow<2.0,>=1.14 in /usr/local/lib/python3.6/dist-packages (1.15.5)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (1.15.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (0.2.2)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (1.15.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (0.36.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (0.8.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (3.14.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (1.32.0)\n",
      "Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (0.2.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (0.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (1.12.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.0,>=1.14) (1.16.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14) (51.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14) (3.3.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.14) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"kashgari>=1.1,<2.0\"\n",
    "!pip install \"tensorflow>=1.14,<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '.'\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Preprocess import *\n",
    "from output_format import *\n",
    "import kashgari\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DXk6ETAnnxLP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences : 4855\n",
      "Number of words in first sentence : 82\n",
      "Number of test article: 159\n"
     ]
    }
   ],
   "source": [
    "#Train data\n",
    "train_2 = f'{folder_path}/train_2.txt'\n",
    "segment_text_list_2, BIO_list_2 = CRF_format(train_2)\n",
    "x_train, y_train = split_sentence(segment_text_list_2, BIO_list_2, 80)\n",
    "\n",
    "#Contest data\n",
    "test = f'{folder_path}/test.txt'\n",
    "test_data = loadInputFile_devData(test)\n",
    "x_contest = [list(t) for t in test_data]\n",
    "x_contest = split_sentence_tst(x_contest, 80)\n",
    "\n",
    "print(\"Number of sentences :\", len(x_train))\n",
    "print(\"Number of words in first sentence :\", len(x_train[0]))\n",
    "print(\"Number of test article:\",len(x_contest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in Training: 2421\n",
      "Number of sentences in Validation: 303\n",
      "Number of sentences in Test: 303\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = delete_some_sentence(x_train, y_train)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split( x_train, y_train, test_size=0.2, random_state=24)\n",
    "x_valid, x_test, y_valid, y_test = train_test_split( x_valid, y_valid, test_size=0.5, random_state=38)\n",
    "print(\"Number of sentences in Training:\", len(x_train))\n",
    "print(\"Number of sentences in Validation:\", len(x_valid))\n",
    "print(\"Number of sentences in Test:\", len(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXA2HB7ui9OL",
    "outputId": "c8cbcfc5-cb33-4fc1-edd3-250636b0945a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['醫', '師', '：', '好', '，', '所', '以', '他', '是', '感', '染', '者', '嗎', '？', '還', '是', '不', '，', '不', '一', '定', '是', '？', '不', '是', '？', '民', '眾', '：', '我', '之', '前', '就', '是', '有', '跟', '他', '去', '驗', '過', '，', '啊', '他', '説', '不', '是', '，', '就', '…', '…', '醫', '師', '：', '就', '是', '去', '匿', '篩', '就', '是', '了', '？', '民', '眾', '：', '對', '。', '醫', '師', '：', '哦', '，', 'o', 'k', '。', '不', '過', '至', '少', '就', '是', '，']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['醫', '師', '：', '哦', '他', '要', '調', '整', '姿', '勢', '。', '民', '眾', '：', '對', '啊', '。', '醫', '師', '：', '超', '音', '波', '。', '民', '眾', '：', '啊', '他', '說', '，', '第', '二', '次', '要', '做', '那', '個', '麻', '醉', '要', '再', '排', '多', '久', '啊', '。', '醫', '師', '：', '嘿', '啊', '。', '民', '眾', '：', '我', '想', '說', '算', '了', '不', '然', '麻', '醉', '不', '要', '用', '勇', '敢', '的', '做', '。', '醫', '師', '：', '好', '，', '那', '我', '就', '開', '一', '年', '。']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-time', 'I-time', 'O']\n",
      "['民', '眾', '：', '好', '好', '，', 'o', 'k', '。', '醫', '師', '：', '所', '以', '原', '則', '上', '是', '這', '樣', '。', '民', '眾', '：', '好', '。', '醫', '師', '：', '那', '所', '以', '如', '果', '如', '果', '還', '好', '的', '話', '就', '就', '就', '就', '就', '就', '就', '就', '就', '還', 'o', 'k', '。', '民', '眾', '：', '恩', '好', '。', '醫', '師', '：', '好', '，', '那', '我', '們', '原', '則', '上', '就', '是', '這', '，', '反', '正', '今', '天', '就', '會', '開', '一', '瓶', '藥', '。']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-time', 'I-time', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "print(x_valid[0])\n",
    "print(y_valid[0])\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construct and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L4R5OUn-pCmw",
    "outputId": "77b8270d-b55f-40ae-8b14-e5e5eb221b1c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:seq_len: 128\n"
     ]
    }
   ],
   "source": [
    "from kashgari.embeddings import BERTEmbedding\n",
    "bert_embed = BERTEmbedding(f'{folder_path}/chinese_L-12_H-768_A-12', task=kashgari.LABELING, sequence_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDvq4gifheid",
    "outputId": "57edf4d7-5357-4f03-f570-e73561845f7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 200, 768), ( 16226304    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 200, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 200, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 200, 768)     153600      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 200, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 200, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 200, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 200, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 200, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 200, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 200, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 200, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 200, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 200, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 200, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 200, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 200, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 200, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 200, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 200, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 200, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 200, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 200, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 200, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 200, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 200, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 200, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 200, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 200, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 200, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 200, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 200, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 200, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 200, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 200, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 200, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 200, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 200, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 200, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 200, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 200, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 200, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 200, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 200, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 200, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 200, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 200, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 200, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 200, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 200, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 200, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 200, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 200, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 200, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 200, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 200, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 200, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 200, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 200, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 200, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 200, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 200, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 200, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 200, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 200, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 200, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 200, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 200, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 200, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 200, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 200, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 200, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 200, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 200, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 200, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 200, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 200, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 200, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 200, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 200, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 200, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 200, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 200, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 200, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 200, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 200, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Output (Concatenate)    (None, 200, 3072)    0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "non_masking_layer (NonMaskingLa (None, 200, 3072)    0           Encoder-Output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "layer_blstm (Bidirectional)     (None, 200, 256)     3277824     non_masking_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_dense (Dense)             (None, 200, 64)      16448       layer_blstm[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_crf_dense (Dense)         (None, 200, 28)      1820        layer_dense[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer_crf (CRF)                 (None, 200, 28)      784         layer_crf_dense[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 104,734,316\n",
      "Trainable params: 3,296,876\n",
      "Non-trainable params: 101,437,440\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 19.2079 - accuracy: 0.9793Epoch 1/25\n",
      "121/121 [==============================] - 526s 4s/step - loss: 19.0733 - accuracy: 0.9794 - val_loss: 486.9846 - val_accuracy: 0.9915\n",
      "Epoch 2/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 4.6427 - accuracy: 0.9929Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 5:00 - loss: 484.5246 - accuracy: 0.9938\n",
      "epoch: 1 precision: 0.621444, recall: 0.696078, f1: 0.656647\n",
      "121/121 [==============================] - 565s 5s/step - loss: 4.7010 - accuracy: 0.9929 - val_loss: 484.5246 - val_accuracy: 0.9938\n",
      "Epoch 3/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 3.2627 - accuracy: 0.9948Epoch 1/25\n",
      "121/121 [==============================] - 479s 4s/step - loss: 3.2439 - accuracy: 0.9948 - val_loss: 482.0275 - val_accuracy: 0.9934\n",
      "Epoch 4/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 2.6138 - accuracy: 0.9955Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:41 - loss: 479.8088 - accuracy: 0.9947\n",
      "epoch: 3 precision: 0.664609, recall: 0.791667, f1: 0.722595\n",
      "121/121 [==============================] - 536s 4s/step - loss: 2.6228 - accuracy: 0.9955 - val_loss: 479.8088 - val_accuracy: 0.9947\n",
      "Epoch 5/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 2.1816 - accuracy: 0.9961Epoch 1/25\n",
      "121/121 [==============================] - 521s 4s/step - loss: 2.1752 - accuracy: 0.9961 - val_loss: 478.5276 - val_accuracy: 0.9948\n",
      "Epoch 6/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.8533 - accuracy: 0.9965Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:55 - loss: 477.0538 - accuracy: 0.9940\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "epoch: 5 precision: 0.724880, recall: 0.742647, f1: 0.733656\n",
      "121/121 [==============================] - 564s 5s/step - loss: 1.8555 - accuracy: 0.9965 - val_loss: 477.0538 - val_accuracy: 0.9940\n",
      "Epoch 7/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.4589 - accuracy: 0.9974Epoch 1/25\n",
      "121/121 [==============================] - 520s 4s/step - loss: 1.4564 - accuracy: 0.9974 - val_loss: 476.3454 - val_accuracy: 0.9951\n",
      "Epoch 8/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.3075 - accuracy: 0.9978Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:56 - loss: 476.3692 - accuracy: 0.9951\n",
      "epoch: 7 precision: 0.746512, recall: 0.786765, f1: 0.766110\n",
      "121/121 [==============================] - 563s 5s/step - loss: 1.3000 - accuracy: 0.9978 - val_loss: 476.3692 - val_accuracy: 0.9951\n",
      "Epoch 9/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.2363 - accuracy: 0.9979Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:52 - loss: 475.8582 - accuracy: 0.9948\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "121/121 [==============================] - 520s 4s/step - loss: 1.2318 - accuracy: 0.9979 - val_loss: 475.8582 - val_accuracy: 0.9948\n",
      "Epoch 10/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1579 - accuracy: 0.9981Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:55 - loss: 475.8614 - accuracy: 0.9951\n",
      "epoch: 9 precision: 0.752358, recall: 0.781863, f1: 0.766827\n",
      "121/121 [==============================] - 563s 5s/step - loss: 1.1554 - accuracy: 0.9981 - val_loss: 475.8614 - val_accuracy: 0.9951\n",
      "Epoch 11/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1647 - accuracy: 0.9981Epoch 1/25\n",
      "121/121 [==============================] - 519s 4s/step - loss: 1.1585 - accuracy: 0.9981 - val_loss: 475.5206 - val_accuracy: 0.9952\n",
      "Epoch 12/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1496 - accuracy: 0.9982Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:56 - loss: 475.9959 - accuracy: 0.9950\n",
      "epoch: 11 precision: 0.752969, recall: 0.776961, f1: 0.764777\n",
      "121/121 [==============================] - 563s 5s/step - loss: 1.1467 - accuracy: 0.9982 - val_loss: 475.9959 - val_accuracy: 0.9950\n",
      "Epoch 13/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1392 - accuracy: 0.9981Epoch 1/25\n",
      "121/121 [==============================] - 520s 4s/step - loss: 1.1423 - accuracy: 0.9981 - val_loss: 474.7516 - val_accuracy: 0.9957\n",
      "Epoch 14/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1395 - accuracy: 0.9982Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:56 - loss: 474.8349 - accuracy: 0.9952\n",
      "epoch: 13 precision: 0.752969, recall: 0.776961, f1: 0.764777\n",
      "121/121 [==============================] - 563s 5s/step - loss: 1.1384 - accuracy: 0.9982 - val_loss: 474.8349 - val_accuracy: 0.9952\n",
      "Epoch 15/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1220 - accuracy: 0.9982Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:54 - loss: 475.7908 - accuracy: 0.9950\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "121/121 [==============================] - 520s 4s/step - loss: 1.1318 - accuracy: 0.9982 - val_loss: 475.7908 - val_accuracy: 0.9950\n",
      "Epoch 16/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1270 - accuracy: 0.9982Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:57 - loss: 476.3092 - accuracy: 0.9953\n",
      "epoch: 15 precision: 0.752969, recall: 0.776961, f1: 0.764777\n",
      "121/121 [==============================] - 563s 5s/step - loss: 1.1344 - accuracy: 0.9982 - val_loss: 476.3092 - val_accuracy: 0.9953\n",
      "Epoch 17/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1277 - accuracy: 0.9983Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:56 - loss: 475.6796 - accuracy: 0.9952\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "121/121 [==============================] - 520s 4s/step - loss: 1.1241 - accuracy: 0.9983 - val_loss: 475.6796 - val_accuracy: 0.9952\n",
      "Epoch 18/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1137 - accuracy: 0.9982Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 5:00 - loss: 475.0312 - accuracy: 0.9955\n",
      "epoch: 17 precision: 0.752969, recall: 0.776961, f1: 0.764777\n",
      "121/121 [==============================] - 563s 5s/step - loss: 1.1112 - accuracy: 0.9982 - val_loss: 475.0312 - val_accuracy: 0.9955\n",
      "Epoch 19/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1121 - accuracy: 0.9982Epoch 1/25\n",
      "121/121 [==============================] - 520s 4s/step - loss: 1.1138 - accuracy: 0.9982 - val_loss: 475.8057 - val_accuracy: 0.9950\n",
      "Epoch 20/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1044 - accuracy: 0.9983Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:59 - loss: 476.1049 - accuracy: 0.9948\n",
      "epoch: 19 precision: 0.752969, recall: 0.776961, f1: 0.764777\n",
      "121/121 [==============================] - 564s 5s/step - loss: 1.0997 - accuracy: 0.9983 - val_loss: 476.1049 - val_accuracy: 0.9948\n",
      "Epoch 21/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1034 - accuracy: 0.9982Epoch 1/25\n",
      "121/121 [==============================] - 521s 4s/step - loss: 1.1029 - accuracy: 0.9982 - val_loss: 475.8823 - val_accuracy: 0.9956\n",
      "Epoch 22/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1177 - accuracy: 0.9982Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 4:59 - loss: 475.7669 - accuracy: 0.9948\n",
      "epoch: 21 precision: 0.752969, recall: 0.776961, f1: 0.764777\n",
      "121/121 [==============================] - 563s 5s/step - loss: 1.1160 - accuracy: 0.9982 - val_loss: 475.7669 - val_accuracy: 0.9948\n",
      "Epoch 23/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1053 - accuracy: 0.9983Epoch 1/25\n",
      "121/121 [==============================] - 521s 4s/step - loss: 1.1062 - accuracy: 0.9983 - val_loss: 475.8012 - val_accuracy: 0.9950\n",
      "Epoch 24/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1138 - accuracy: 0.9982Epoch 1/25\n",
      " 16/121 [==>...........................] - ETA: 5:00 - loss: 475.6804 - accuracy: 0.9956\n",
      "epoch: 23 precision: 0.752969, recall: 0.776961, f1: 0.764777\n",
      "121/121 [==============================] - 566s 5s/step - loss: 1.1066 - accuracy: 0.9982 - val_loss: 475.6804 - val_accuracy: 0.9956\n",
      "Epoch 25/25\n",
      "120/121 [============================>.] - ETA: 3s - loss: 1.1070 - accuracy: 0.9983Epoch 1/25\n",
      "121/121 [==============================] - 522s 4s/step - loss: 1.1006 - accuracy: 0.9983 - val_loss: 475.6183 - val_accuracy: 0.9958\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
    "from kashgari.callbacks import EvalCallBack\n",
    "from kashgari.tasks.labeling import BiLSTM_CRF_Model,BiLSTM_Model,CNN_LSTM_Model\n",
    "model = BiLSTM_CRF_Model(bert_embed)\n",
    "stop_callback = EarlyStopping(monitor='val_accuracy',patience=5, restore_best_weights=True)\n",
    "eval_callback = EvalCallBack(kash_model=model,valid_x=x_valid,valid_y=y_valid,step=2)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, verbose=1, min_lr=1e-6)\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=24,\n",
    "                    x_validate = x_valid, \n",
    "                    y_validate = y_valid, \n",
    "                    callbacks = [reduce_lr,eval_callback],\n",
    "                    shuffle = True)\n",
    "model.save(f'{folder_path}/BILSTMCRF_googlebert_batch24_sepwithmark_to80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "abpVrhQ6VxAR",
    "outputId": "5588b243-b9f5-42ac-9c1c-13eeb8fca8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      location     0.8378    0.8158    0.8267        38\n",
      "        family     1.0000    0.1250    0.2222         8\n",
      "          time     0.7744    0.8548    0.8126       241\n",
      "      med_exam     0.8269    0.9149    0.8687        47\n",
      "          name     0.8537    0.8140    0.8333        43\n",
      "         money     0.8333    0.9091    0.8696        11\n",
      "       contact     0.0000    0.0000    0.0000         5\n",
      "    profession     0.0000    0.0000    0.0000         9\n",
      "            ID     0.0000    0.0000    0.0000         5\n",
      "clinical_event     0.0000    0.0000    0.0000         1\n",
      "\n",
      "     micro avg     0.7951    0.7990    0.7971       408\n",
      "     macro avg     0.7628    0.7990    0.7727       408\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                precision    recall  f1-score   support\\n\\n      location     0.8378    0.8158    0.8267        38\\n        family     1.0000    0.1250    0.2222         8\\n          time     0.7744    0.8548    0.8126       241\\n      med_exam     0.8269    0.9149    0.8687        47\\n          name     0.8537    0.8140    0.8333        43\\n         money     0.8333    0.9091    0.8696        11\\n       contact     0.0000    0.0000    0.0000         5\\n    profession     0.0000    0.0000    0.0000         9\\n            ID     0.0000    0.0000    0.0000         5\\nclinical_event     0.0000    0.0000    0.0000         1\\n\\n     micro avg     0.7951    0.7990    0.7971       408\\n     macro avg     0.7628    0.7990    0.7727       408\\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Contest Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
     ]
    }
   ],
   "source": [
    "# Choose trained model\n",
    "model_name = \"/BILSTMCRF_googlebert_batch24_sepwithmark_to80\"\n",
    "#model_name = \"/BILSTMCRF_googlebert_batch36_sepwithmark_to80\"\n",
    "#model_name = \"/BILSTMCRF_roberta_wwm_large_ext_L-24_H-1024_A-16_batch32_sepwithmark_to80\"\n",
    "#model_name = \"/BILSTMCRF_wwm_ext_bert_batch24_sepwithmark_to80\"\n",
    "path = folder_path + model_name\n",
    "model = kashgari.utils.load_model(f'{path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Predict article  0\n",
      "Finish Predict article  1\n",
      "Finish Predict article  2\n",
      "Finish Predict article  3\n",
      "Finish Predict article  4\n",
      "Finish Predict article  5\n",
      "Finish Predict article  6\n",
      "Finish Predict article  7\n",
      "Finish Predict article  8\n",
      "Finish Predict article  9\n",
      "Finish Predict article  10\n",
      "Finish Predict article  11\n",
      "Finish Predict article  12\n",
      "Finish Predict article  13\n",
      "Finish Predict article  14\n",
      "Finish Predict article  15\n",
      "Finish Predict article  16\n",
      "Finish Predict article  17\n",
      "Finish Predict article  18\n",
      "Finish Predict article  19\n",
      "Finish Predict article  20\n",
      "Finish Predict article  21\n",
      "Finish Predict article  22\n",
      "Finish Predict article  23\n",
      "Finish Predict article  24\n",
      "Finish Predict article  25\n",
      "Finish Predict article  26\n",
      "Finish Predict article  27\n",
      "Finish Predict article  28\n",
      "Finish Predict article  29\n",
      "Finish Predict article  30\n",
      "Finish Predict article  31\n",
      "Finish Predict article  32\n",
      "Finish Predict article  33\n",
      "Finish Predict article  34\n",
      "Finish Predict article  35\n",
      "Finish Predict article  36\n",
      "Finish Predict article  37\n",
      "Finish Predict article  38\n",
      "Finish Predict article  39\n",
      "Finish Predict article  40\n",
      "Finish Predict article  41\n",
      "Finish Predict article  42\n",
      "Finish Predict article  43\n",
      "Finish Predict article  44\n",
      "Finish Predict article  45\n",
      "Finish Predict article  46\n",
      "Finish Predict article  47\n",
      "Finish Predict article  48\n",
      "Finish Predict article  49\n",
      "Finish Predict article  50\n",
      "Finish Predict article  51\n",
      "Finish Predict article  52\n",
      "Finish Predict article  53\n",
      "Finish Predict article  54\n",
      "Finish Predict article  55\n",
      "Finish Predict article  56\n",
      "Finish Predict article  57\n",
      "Finish Predict article  58\n",
      "Finish Predict article  59\n",
      "Finish Predict article  60\n",
      "Finish Predict article  61\n",
      "Finish Predict article  62\n",
      "Finish Predict article  63\n",
      "Finish Predict article  64\n",
      "Finish Predict article  65\n",
      "Finish Predict article  66\n",
      "Finish Predict article  67\n",
      "Finish Predict article  68\n",
      "Finish Predict article  69\n",
      "Finish Predict article  70\n",
      "Finish Predict article  71\n",
      "Finish Predict article  72\n",
      "Finish Predict article  73\n",
      "Finish Predict article  74\n",
      "Finish Predict article  75\n",
      "Finish Predict article  76\n",
      "Finish Predict article  77\n",
      "Finish Predict article  78\n",
      "Finish Predict article  79\n",
      "Finish Predict article  80\n",
      "Finish Predict article  81\n",
      "Finish Predict article  82\n",
      "Finish Predict article  83\n",
      "Finish Predict article  84\n",
      "Finish Predict article  85\n",
      "Finish Predict article  86\n",
      "Finish Predict article  87\n",
      "Finish Predict article  88\n",
      "Finish Predict article  89\n",
      "Finish Predict article  90\n",
      "Finish Predict article  91\n",
      "Finish Predict article  92\n",
      "Finish Predict article  93\n",
      "Finish Predict article  94\n",
      "Finish Predict article  95\n",
      "Finish Predict article  96\n",
      "Finish Predict article  97\n",
      "Finish Predict article  98\n",
      "Finish Predict article  99\n",
      "Finish Predict article  100\n",
      "Finish Predict article  101\n",
      "Finish Predict article  102\n",
      "Finish Predict article  103\n",
      "Finish Predict article  104\n",
      "Finish Predict article  105\n",
      "Finish Predict article  106\n",
      "Finish Predict article  107\n",
      "Finish Predict article  108\n",
      "Finish Predict article  109\n",
      "Finish Predict article  110\n",
      "Finish Predict article  111\n",
      "Finish Predict article  112\n",
      "Finish Predict article  113\n",
      "Finish Predict article  114\n",
      "Finish Predict article  115\n",
      "Finish Predict article  116\n",
      "Finish Predict article  117\n",
      "Finish Predict article  118\n",
      "Finish Predict article  119\n",
      "Finish Predict article  120\n",
      "Finish Predict article  121\n",
      "Finish Predict article  122\n",
      "Finish Predict article  123\n",
      "Finish Predict article  124\n",
      "Finish Predict article  125\n",
      "Finish Predict article  126\n",
      "Finish Predict article  127\n",
      "Finish Predict article  128\n",
      "Finish Predict article  129\n",
      "Finish Predict article  130\n",
      "Finish Predict article  131\n",
      "Finish Predict article  132\n",
      "Finish Predict article  133\n",
      "Finish Predict article  134\n",
      "Finish Predict article  135\n",
      "Finish Predict article  136\n",
      "Finish Predict article  137\n",
      "Finish Predict article  138\n",
      "Finish Predict article  139\n",
      "Finish Predict article  140\n",
      "Finish Predict article  141\n",
      "Finish Predict article  142\n",
      "Finish Predict article  143\n",
      "Finish Predict article  144\n",
      "Finish Predict article  145\n",
      "Finish Predict article  146\n",
      "Finish Predict article  147\n",
      "Finish Predict article  148\n",
      "Finish Predict article  149\n",
      "Finish Predict article  150\n",
      "Finish Predict article  151\n",
      "Finish Predict article  152\n",
      "Finish Predict article  153\n",
      "Finish Predict article  154\n",
      "Finish Predict article  155\n",
      "Finish Predict article  156\n",
      "Finish Predict article  157\n",
      "Finish Predict article  158\n"
     ]
    }
   ],
   "source": [
    "make_contest_output(x_contest, model, path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "kashgari",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
